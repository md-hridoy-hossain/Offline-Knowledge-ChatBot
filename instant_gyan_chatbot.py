# -*- coding: utf-8 -*-
"""Instant-Gyan-Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19LsSRpVpnPiAgAhpcu58bzMGJ93ABxOI
"""

!pip install -q torch transformers accelerate gradio sentencepiece

import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load the model
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.float16)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# 2. Optimized Answer Generation
def generate_answer(question):
    try:
        inputs = tokenizer(
            f"Explain in detail: {question}",
            return_tensors="pt",
            max_length=512,
            truncation=True
        ).to(model.device)

        outputs = model.generate(
            **inputs,
            max_new_tokens=250,  # Optimal length
            num_beams=3,
            early_stopping=True,
            do_sample=False  # Faster generation
        )
        return tokenizer.decode(outputs[0], skip_special_tokens=True)
    except torch.cuda.OutOfMemoryError:
        torch.cuda.empty_cache()
        return "‚ö†Ô∏è Memory error - Please ask a shorter question"
    except Exception as e:
        return f"‚ö†Ô∏è Error: {str(e)}"

# 3. Gradio Interface with Real-Time Updates
with gr.Blocks(title="Gyan Chatbot") as demo:
    gr.Markdown("## üî¨ Instant Science Answers (Offline)")

    with gr.Row():
        chatbot = gr.Chatbot(height=400, label="Conversation")
        with gr.Column():
            msg = gr.Textbox(label="Your Question", placeholder="Ask about science, health, tech...")
            with gr.Row():
                submit_btn = gr.Button("Submit", variant="primary")
                clear_btn = gr.Button("Clear")

    # Improved response handling
    def respond(question, chat_history):
        if not question.strip():
            return chat_history, "Please enter a question"

        # Immediate feedback
        chat_history.append((question, "Generating answer..."))
        yield chat_history, ""

        # Get answer
        answer = generate_answer(question)

        # Update final answer
        chat_history[-1] = (question, answer)
        yield chat_history, ""

    # Event handlers
    msg.submit(respond, [msg, chatbot], [chatbot, msg])
    submit_btn.click(respond, [msg, chatbot], [chatbot, msg])
    clear_btn.click(lambda: [], None, chatbot)

# 4. Launch with Resource Monitoring
def launch_app():
    try:
        demo.launch(share=True, debug=True)
    except Exception as e:
        print(f"Launch error: {e}")
        print("Try: Runtime ‚Üí Restart runtime ‚Üí Run again")

# Check GPU memory first
if torch.cuda.is_available():
    print(f"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB used")
    launch_app()
else:
    raise gr.Error("GPU not available - Enable T4 GPU in Colab")